# -*- coding: utf-8 -*-
"""FeedbackPrize2-v12a-train-01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13hLuN1DloOASTrpcR0-mqNqrlTMEb--m

# Version v1a
"""
"""# Packages"""

import os, gc, pickle, math, time, random, copy, json
from glob import glob
import numpy as np
import pandas as pd
from tqdm import tqdm
import multiprocessing

from sklearn.model_selection import GroupKFold, StratifiedGroupKFold
from sklearn.metrics import log_loss

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from datasets import Dataset

# Mixed precision in Pytorch
from torch.cuda.amp import autocast, GradScaler

# For SWA
from torch.optim.swa_utils import AveragedModel, SWALR

from transformers import AutoConfig, AutoTokenizer, AutoModel
from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, AdamW

import warnings
warnings.filterwarnings('ignore')

os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'true'

"""# Initial configuration"""

class Config():
    # General settings
    competition_name = 'FeedbackPrize2'
    env = 'local'
    seed = 1
    mode = 'train'    # 'train', 'valid'
    debug = False
    model_name = 'v1a'
    processed_data = True    # Set to True if the train_df is processed and added with the target (rank)
    use_tqdm = True
    # For model
    backbone = 'microsoft/deberta-v3-large'
    tokenizer = AutoTokenizer.from_pretrained(backbone)
    config = AutoConfig.from_pretrained(backbone)
    config.output_hidden_states = True
    config.hidden_dropout_prob = 0.
    config.attention_probs_dropout_prob = 0.
    # Add new token
    cls_token = '[FP2]'
    special_tokens_dict = {'additional_special_tokens': [cls_token]}
    tokenizer.add_special_tokens(special_tokens_dict)
    cls_token_id = tokenizer(cls_token)['input_ids'][1]
    # For data
    discourse_type_map = {
        'Lead': 0,
        'Position': 1,
        'Claim': 2,
        'Counterclaim': 3,
        'Rebuttal': 4,
        'Evidence': 5,
        'Concluding Statement': 6,
    }
    label_map = {
        'Ineffective': 0,
        'Adequate': 1,
        'Effective': 2,
        'None': -100,
    }
    training_folds = [0]
    max_len = 1024
    batch_size = 4
    num_workers = os.cpu_count()
    # For training
    apex = True
    gradient_checkpointing = True
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    nepochs = 5
    start_val_epoch = 2
    val_check_interval = 0.1
    gradient_accumulation_steps = 1.
    max_grad_norm = 1000
    label_smoothing = 0.03
    # For SWA
    use_swa = False
    if use_swa:
        start_swa_epoch = 2
        swa_lr = 5e-6
        swa_anneal_strategy = 'cos'
        swa_anneal_epochs = 1
    else:
        start_swa_epoch = nepochs + 1
    # For AWP
    use_awp = True
    if use_awp:
        start_awp_epoch = 2
        adv_lr = 1e-5
        adv_eps = 1e-3
        adv_step = 1
    else:
        start_awp_epoch = nepochs + 1
    # Optimizer
    lr = 1e-5
    weight_decay = 1e-2
    encoder_lr = 1e-5
    decoder_lr = 1e-5
    min_lr = 1e-6
    eps = 1e-6
    betas = (0.9, 0.999)
    # Scheduler
    scheduler_type = 'cosine'    # 'linear', 'cosine'
    if scheduler_type == 'cosine':
        num_cycles = 0.5
    num_warmup_steps = 100
    batch_scheduler = True
    # Directories
    if env == 'colab':
        comp_data_dir = f'/content/drive/My Drive/Kaggle competitions/{competition_name}/comp_data'
        extra_data_dir = f'/content/drive/My Drive/Kaggle competitions/{competition_name}/extra_data'
        model_dir = f'/content/drive/My Drive/Kaggle competitions/{competition_name}/model'
        os.makedirs(os.path.join(model_dir, model_name.split('_')[0][:-1], model_name.split('_')[0][-1]), exist_ok = True)
        old_comp_data_dir = f'/content/drive/My Drive/Kaggle competitions/{competition_name[:-1]}/data'
    elif env == 'kaggle':
        comp_data_dir = ...
        extra_data_dir = ...
        model_dir = ...
    elif env == 'vastai':
        comp_data_dir = 'data'
        extra_data_dir = 'data'
        model_dir = 'model'
        os.makedirs(os.path.join(model_dir, model_name.split('_')[0][:-1], model_name.split('_')[0][-1]), exist_ok = True)
    elif env == 'local':
        comp_data_dir = 'data'
        extra_data_dir = 'ext_data'
        model_dir = 'model'
        os.makedirs(os.path.join(model_dir, model_name.split('_')[0][:-1], model_name.split('_')[0][-1]), exist_ok = True)

cfg = Config()

"""# Random seed"""

def set_random_seed(seed, use_cuda = True):
    np.random.seed(seed) # cpu vars
    torch.manual_seed(seed) # cpu  vars
    random.seed(seed) # Python
    os.environ['PYTHONHASHSEED'] = str(seed) # Python hash building
    if use_cuda:
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed) # gpu vars
        torch.backends.cudnn.deterministic = True  #needed
        torch.backends.cudnn.benchmark = False
        
set_random_seed(cfg.seed)

"""# Set-up log"""

import logging
from imp import reload
reload(logging)
logging.basicConfig(
    level = logging.INFO,
    format = '%(asctime)s %(message)s',
    datefmt = '%H:%M:%S',
    handlers = [
        logging.FileHandler(f"train_{cfg.model_name}_{time.strftime('%m%d_%H%M', time.localtime())}_{cfg.seed}.log"),
        logging.StreamHandler()
    ]
)

logging.info(
    '\nmodel_name: {}\n'
    'env: {}\n'
    'seed: {}\n'
    'training_folds: {}\n'
    'max_len: {}\n'
    'batch_size: {}\n'
    'num_workers: {}\n'
    'nepochs: {}\n'
    'lr: {}\n'
    'weight_decay: {}\n'
    'gradient_accumulation_steps: {}'.format(cfg.model_name, cfg.env, cfg.seed, cfg.training_folds, cfg.max_len,
                                             cfg.batch_size, cfg.num_workers, cfg.nepochs, cfg.lr, cfg.weight_decay, 
                                             cfg.gradient_accumulation_steps)
)

"""# Import data

* Train data
"""

if cfg.processed_data:
    train = pd.read_csv(os.path.join(cfg.extra_data_dir, f'train_seed_{cfg.seed}.csv'))
else:
    train = pd.read_csv(os.path.join(cfg.comp_data_dir, 'train.csv'))
train

# Read each easy
if cfg.processed_data:
    essay_df = pd.read_csv(os.path.join(cfg.extra_data_dir, 'essay_df.csv'))
else:
    essay_paths = glob(f'{cfg.comp_data_dir}/train/*.txt')
    essay_dict = {
        'essay_id': [i.split('/')[-1].split('.')[0] for i in tqdm(essay_paths)],
        'content': [open(i).read() for i in tqdm(essay_paths)],
    }
    essay_df = pd.DataFrame.from_dict(essay_dict)
    essay_df.to_csv(os.path.join(cfg.extra_data_dir, 'essay_df.csv'), index = None)

assert essay_df.shape[0] == train.essay_id.nunique()

"""* Load data from the old FeedbackPrize competition"""

if not cfg.processed_data:
    old_train = pd.read_pickle(os.path.join(cfg.old_comp_data_dir, 'df.pkl'))
    try:
        detail_essay_df = old_train.set_index('id').loc[essay_df.essay_id]
        print('All essays in the new competition are in the dataset of the old competition!')
        print('Ratio: {:.4f}%'.format(detail_essay_df.shape[0] / old_train.shape[0] * 100))
    except:
        print('Some essays in the new competition are NOT in the dataset of the old competition!')

"""* Merge the two data sets"""

if not cfg.processed_data:
    new_train = []
    for id, df in tqdm(train.groupby('essay_id')):
        assert df.shape[0] == len(detail_essay_df.loc[id, 'starts'])
        df['starts'] = detail_essay_df.loc[id, 'starts']
        df['ends'] = detail_essay_df.loc[id, 'ends']
        df['predictionstrings'] = detail_essay_df.loc[id, 'predictionstrings']
        df['text'] = detail_essay_df.loc[id, 'text']
        new_train.append(df)

    train = pd.concat(new_train)
    train.reset_index(drop = True, inplace = True)

"""# Encode labels and discourse_type"""

if not cfg.processed_data:
    train['label'] = train['discourse_effectiveness'].map(cfg.label_map)

"""# CV split"""

if not cfg.processed_data:
    split = StratifiedGroupKFold(n_splits = 5, shuffle = True, random_state = cfg.seed)
    train['fold'] = -1
    for i, (trn_idx, val_idx) in enumerate(split.split(train, train['label'], groups = train['essay_id'])):
        train.loc[val_idx, 'fold'] = i
    train.to_csv(os.path.join(cfg.extra_data_dir, f'train_seed_{cfg.seed}.csv'), index = None)

"""# Reformulate the train data so that entire a text is processed at a whole"""

def organize_df(df, id, cfg):
    discourse_ids = '|'.join(df['discourse_id'].tolist())
    discourses = ''.join((cfg.cls_token + df['discourse_type'] + '. ' + df['discourse_text']).tolist())
    text = df['text'].unique()[0]
    discourse_type = '|'.join(df['discourse_type'].tolist())
    label_list = '|'.join(df['discourse_effectiveness'].tolist())
    fold = df['fold'].unique()[0]
    return pd.DataFrame({
        'discourse_ids': discourse_ids,
        'discourses': discourses,
        'text': text,
        'discourse_type': discourse_type,
        'label_list': label_list,
        'fold': fold,
    }, index = [id])

new_df = []
for id, df in tqdm(train.groupby('essay_id')):
    new_df.append(organize_df(df, id, cfg))
train_df = pd.concat(new_df).reset_index()
if cfg.debug:
    train_df = train_df.iloc[:100]
    cfg.batch_size = 4

"""# Dataset"""

def create_label(cfg, input_ids, overflow_to_sample_mapping, label_list):
    labels = []
    is_tail = []
    stay_in_old = 0
    for i, (input_id, sample_mapping) in enumerate(zip(input_ids, overflow_to_sample_mapping)):
        input_ids_array = np.array(input_id)
        label = np.zeros_like(input_id) - 100
        if i > 0:
            past_sample_mapping = overflow_to_sample_mapping[i-1]
        else:
            past_sample_mapping = -1

        num_cls_tokens = sum(input_ids_array == cfg.cls_token_id)
        if sample_mapping == past_sample_mapping:
            # In this case, we are still in the old text, then the first CLS token is a fake location for label
            stay_in_old += 1
            start = old_end
            is_tail.append(1)
        else:
            stay_in_old = 0
            start = 0
            is_tail.append(0)
        
        end = start + num_cls_tokens
        old_end = end
        
        chosen_tokens = input_ids_array == cfg.cls_token_id
        label[chosen_tokens] = label_list[sample_mapping][start:end]
        labels.append(label)

    return labels, is_tail

def prepare_dataset(example, cfg = cfg, mode = 'train'):
    discourse_ids = example['discourse_ids']
    discourses = example['discourses']
    text = example['text']
    discourse_type = example['discourse_type']
    label_list = [[cfg.label_map[i] for i in l.split('|')] for l in example['label_list']]

    tokenized_discourses = cfg.tokenizer(discourses, return_attention_mask = True, truncation = True, padding = 'max_length', max_length = cfg.max_len, return_overflowing_tokens = True)
    input_ids = tokenized_discourses['input_ids']
    overflow_to_sample_mapping = tokenized_discourses['overflow_to_sample_mapping']
    assert max(overflow_to_sample_mapping) == len(label_list) - 1

    labels, is_tail = create_label(cfg, input_ids, overflow_to_sample_mapping, label_list)
    tokenized_discourses['labels'] = labels
    tokenized_discourses['is_tail'] = is_tail

    return tokenized_discourses

def generate_dataset(cfg, df, mode = 'train'):
    ds = Dataset.from_pandas(df)
    tokenized_ds = ds.map(prepare_dataset, fn_kwargs = {'cfg': cfg, 'mode': mode}, batched = True, batch_size = 10_000, remove_columns = ds.column_names)
    return tokenized_ds

class FeedbackPrize2_Dataset(Dataset):
    def __init__(self, cfg, df, mode = 'train'):
        self.dataset = generate_dataset(cfg, df, mode = mode)
    
    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        return item

"""* Collate function"""

def collate_fn(batch):
    input_ids = []
    attention_mask = []
    token_type_ids = []
    is_tail = []
    labels = []

    for item in batch:
        batch_input_ids = item['input_ids']
        batch_attention_mask = item['attention_mask']
        batch_token_type_ids = item['token_type_ids']
        batch_is_tail = item['is_tail']
        batch_labels = item['labels']

        input_ids.append(batch_input_ids)
        attention_mask.append(batch_attention_mask)
        token_type_ids.append(batch_token_type_ids)
        is_tail.append(batch_is_tail)
        labels.append(batch_labels)

    return {
        'input_ids': torch.tensor(input_ids, dtype = torch.long),
        'attention_mask': torch.tensor(attention_mask, dtype = torch.long),
        'token_type_ids': torch.tensor(token_type_ids, dtype = torch.long),
        'is_tail': torch.tensor(is_tail, dtype = torch.long),
        'labels': torch.tensor(labels, dtype = torch.long),
    }

"""# Model"""

class FeedbackPrize2_Model(nn.Module):
    def __init__(self, cfg):
        super(FeedbackPrize2_Model, self).__init__()
        self.cfg = cfg
        # Backbone
        self.backbone = AutoModel.from_pretrained(cfg.backbone, config = cfg.config)
        self.backbone.resize_token_embeddings(len(cfg.tokenizer))
        if cfg.gradient_checkpointing:
            self.backbone.gradient_checkpointing_enable()

        # Multidropout
        self.dropout1 = nn.Dropout(0.1)
        self.dropout2 = nn.Dropout(0.2)
        self.dropout3 = nn.Dropout(0.3)
        self.dropout4 = nn.Dropout(0.4)
        self.dropout5 = nn.Dropout(0.5)
        
        # Head
        self.head = nn.Linear(cfg.config.hidden_size, 3)
        
        self._init_weights(self.head)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean = 0.0, std = cfg.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean = 0.0, std = cfg.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def criterion(self, pred, true):
        loss = nn.CrossEntropyLoss(ignore_index = -100, label_smoothing = cfg.label_smoothing)(pred.permute(0, 2, 1), true)
        return loss

    def forward(self, input_ids, attention_mask, token_type_ids, label = None):
        output_backbone = self.backbone(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids).last_hidden_state    # batch_size, seq_len, hidden_size
        
        output1 = self.head(self.dropout1(output_backbone))
        output2 = self.head(self.dropout2(output_backbone))
        output3 = self.head(self.dropout3(output_backbone))
        output4 = self.head(self.dropout4(output_backbone))
        output5 = self.head(self.dropout5(output_backbone))

        output = (output1 + output2 + output3 + output4 + output5) / 5

        if label is not None:
            loss = self.criterion(output, label)
        else:
            loss = None
        return loss, output

"""# Util functions

* Metrics
"""

def metric(y_pred, y_true):
    y_pred = y_pred.astype(np.float64)
    return log_loss(y_true, y_pred)

"""* AWP 
https://www.kaggle.com/code/wht1996/feedback-nn-train/notebook
"""

class AWP:
    def __init__(
        self,
        model,
        optimizer,
        adv_param = 'weight',
        adv_lr = 1,
        adv_eps = 0.2,
        start_step = 0,
        adv_step = 1,
        scaler = None
    ):
        self.model = model
        self.optimizer = optimizer
        self.adv_param = adv_param
        self.adv_lr = adv_lr
        self.adv_eps = adv_eps
        self.start_step = start_step
        self.adv_step = adv_step
        self.backup = {}
        self.backup_eps = {}
        self.scaler = scaler

    def attack_backward(self, batch, epoch):
        if (self.adv_lr == 0) or (epoch < self.start_step):
            return None

        self._save()
        for i in range(self.adv_step):
            self._attack_step() 
            with autocast(enabled = cfg.apex):
                input_ids = batch['input_ids'].to(cfg.device)
                attention_mask = batch['attention_mask'].to(cfg.device)
                token_type_ids = batch['token_type_ids'].to(cfg.device)
                labels = batch['labels'].to(cfg.device)
                adv_loss, _  = self.model(input_ids, attention_mask, token_type_ids, labels)
                adv_loss = adv_loss.mean()
            self.optimizer.zero_grad()
            self.scaler.scale(adv_loss).backward()
            
        self._restore()

    def _attack_step(self):
        e = 1e-6
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.grad is not None and self.adv_param in name:
                norm1 = torch.norm(param.grad)
                norm2 = torch.norm(param.data.detach())
                if norm1 != 0 and not torch.isnan(norm1):
                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)
                    param.data.add_(r_at)
                    param.data = torch.min(
                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]
                    )
                    
    def _save(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.grad is not None and self.adv_param in name:
                if name not in self.backup:
                    self.backup[name] = param.data.clone()
                    grad_eps = self.adv_eps * param.abs().detach()
                    self.backup_eps[name] = (
                        self.backup[name] - grad_eps,
                        self.backup[name] + grad_eps,
                    )

    def _restore(self,):
        for name, param in self.model.named_parameters():
            if name in self.backup:
                param.data = self.backup[name]
        self.backup = {}
        self.backup_eps = {}

"""* Helper functions"""

def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))

"""* One-epoch training function"""

def train_fn(cfg, model, train_dataloader, optimizer, epoch, num_train_steps, scheduler, 
             valid_dataloader, val_df, valid, swa_model = None, swa_scheduler = None, best_score = np.inf, fold = 0):
    '''
    val_df: the validation dataframe after re-organizing
    valid: the validation dataframe before re-organizing
    '''
    # Set up for training
    scaler = GradScaler(enabled = cfg.apex)   # Enable APEX
    loss = 0
    total_samples = 0
    global_step = 0
    start = end = time.time()

    if cfg.use_awp:
        # Initialize AWP
        awp = AWP(model, optimizer, adv_lr = cfg.adv_lr, adv_eps = cfg.adv_eps, start_step = cfg.start_awp_epoch, scaler = scaler)

    if cfg.use_tqdm:
        tbar = tqdm(train_dataloader)
    else:
        tbar = train_dataloader
        
    val_schedule = [int(i) for i in list(np.linspace(1, len(tbar), num = int(1 / cfg.val_check_interval) + 1, endpoint = True))[1:]]

    for i, item in enumerate(tbar):
        model.train()
        # Set up inputs
        input_ids = item['input_ids'].to(cfg.device)
        attention_mask = item['attention_mask'].to(cfg.device)
        token_type_ids = item['token_type_ids'].to(cfg.device)
        labels = item['labels'].to(cfg.device)
        
        batch_size = input_ids.shape[0]

        # Forward
        with autocast(enabled = cfg.apex):
            batch_loss, _ = model(input_ids, attention_mask, token_type_ids, labels)

        if cfg.gradient_accumulation_steps > 1:
            batch_loss = batch_loss / cfg.gradient_accumulation_steps

        # Backward
        scaler.scale(batch_loss).backward()       
        
        if cfg.use_awp and epoch >= cfg.start_awp_epoch:
            if epoch == cfg.start_awp_epoch and i == 0:
                logging.info(' Start AWP '.center(50, '-'))
            if (i + 1) % cfg.gradient_accumulation_steps == 0:
                awp.attack_backward(item, epoch)

        # Update loss
        loss += batch_loss.item() * batch_size
        total_samples += batch_size

        if cfg.use_tqdm:
            tbar.set_description('Batch loss: {:.4f} - Avg loss: {:.4f}'.format(batch_loss, loss / total_samples))

        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)
        if (i + 1) % cfg.gradient_accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            global_step += 1
            if swa_model is not None:
                swa_model.update_parameters(model)
                swa_scheduler.step()
            else:
                if cfg.batch_scheduler:
                    scheduler.step()
            optimizer.zero_grad()

        # Evaluate
        if epoch >= cfg.start_val_epoch:
            if (i + 1) in val_schedule:
                logging.info('Epoch: [{0}][{1}/{2}] - Start evaluating...'.format(epoch + 1, i + 1, len(tbar)))
                if swa_model is not None:
                    torch.optim.swa_utils.update_bn(list(train_dataloader)[:(i + 1)], swa_model)
                    val_loss, pred = valid_fn(cfg, swa_model, valid_dataloader)
                else:
                    val_loss, pred = valid_fn(cfg, model, valid_dataloader)

                true = np.array([cfg.label_map[item] for sublist in [i.split('|') for i in val_df['label_list'].tolist()] for item in sublist])
                assert pred.shape[0] == len(true)
                valid[list(cfg.label_map.keys())[:3]] = pred

                # Scoring
                score = metric(pred, true)

                end = time.time()
                logging.info('Epoch: [{0}][{1}/{2}] - '
                            'Elapsed {remain:s} - '
                            'Train Loss: {train_loss:.4f} - '
                            'Val Loss: {val_loss:.4f} - '
                            'Score: {score:.4f} - '
                            'LR: {lr:.8f}'
                            .format(epoch + 1, i + 1, len(tbar), 
                                    remain = timeSince(start, float(i + 1) / len(tbar)),
                                    train_loss = loss / total_samples,
                                    val_loss = val_loss,
                                    score = score,
                                    lr = scheduler.get_lr()[0]))
                if score < best_score:
                    best_score = score
                    logging.info(f'Epoch [{epoch + 1}][{i + 1}/{len(tbar)}] - The Best Score Updated to: {best_score:.4f} Model')
                    if swa_model is None:
                        model_state_dict = {
                            'state_dict': model.state_dict(),
                            'pred': pred,
                        }
                    else:
                        model_state_dict = {
                            'state_dict': swa_model.state_dict(),
                            'pred': pred,
                        }
                    ckp = os.path.join(cfg.model_dir, cfg.model_name.split('_')[0][:-1], cfg.model_name.split('_')[0][-1], f'fold_{fold}.pt')
                    torch.save(model_state_dict, ckp)
                else:
                    logging.info(f'Epoch [{epoch + 1}][{i + 1}/{len(tbar)}] - Not The Best Score ({score:.4f}), Current Best Score: {best_score:.4f} Model')
            
    return best_score, valid

"""* One-epoch validating function"""

def valid_fn(cfg, model, valid_dataloader):
    # Set up for training
    model.eval()

    loss = 0
    total_samples = 0
    start = end = time.time()

    preds = []

    if cfg.use_tqdm:
        tbar = tqdm(valid_dataloader)
    else:
        tbar = valid_dataloader

    for i, item in enumerate(tbar):
        # Set up inputs
        input_ids = item['input_ids'].to(cfg.device)
        attention_mask = item['attention_mask'].to(cfg.device)
        token_type_ids = item['token_type_ids'].to(cfg.device)
        is_tail = item['is_tail']
        labels = item['labels'].to(cfg.device)

        batch_size = input_ids.shape[0]

        # Forward
        with torch.no_grad():
            with autocast(enabled = cfg.apex):
                batch_loss, batch_pred = model(input_ids, attention_mask, token_type_ids, labels)
        
            # Update loss
            loss += batch_loss.item() * batch_size
            total_samples += batch_size

        # Now, locate only the FP2 tokens
        batch_pred = batch_pred[torch.where(input_ids == cfg.cls_token_id)]

        # Store the predictions
        batch_pred = F.softmax(batch_pred, dim = -1)
        preds.append(batch_pred.detach().cpu().numpy())

        # Logging
        end = time.time()
        
    preds = np.concatenate(preds, axis = 0)

    return loss / total_samples, preds

"""* Optimizer and scheduler"""

def get_optimizer(cfg, model):
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    optimizer_parameters = [
        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],
             'lr': cfg.encoder_lr, 'weight_decay': cfg.weight_decay},
        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],
             'lr': cfg.encoder_lr, 'weight_decay': 0.0},
        {'params': [p for n, p in model.named_parameters() if 'backbone' not in n],
             'lr': cfg.decoder_lr, 'weight_decay': 0.0}
    ]
    optimizer = AdamW(optimizer_parameters, lr = cfg.lr, eps = cfg.eps, betas = cfg.betas)
    return optimizer

def get_scheduler(cfg, optimizer, num_train_steps):
    if cfg.scheduler_type == 'linear':
        scheduler = get_linear_schedule_with_warmup(
            optimizer, num_warmup_steps = cfg.num_warmup_steps, num_training_steps = num_train_steps
        )
    elif cfg.scheduler_type == 'cosine':
        scheduler = get_cosine_schedule_with_warmup(
            optimizer, num_warmup_steps = cfg.num_warmup_steps, num_training_steps = num_train_steps, num_cycles = cfg.num_cycles
        )
    return scheduler

"""# Training loop"""

def training_loop(cfg, fold = 0):
    logging.info(f' Fold {fold} '.center(50, '*'))
    set_random_seed(cfg.seed + fold)
    
    logging.info('Preparing training and validating dataloader...')
    trn = train_df[train_df.fold != fold]
    val = train_df[train_df.fold == fold]
    valid = train[train.fold == fold]

    train_dataset = FeedbackPrize2_Dataset(cfg, trn, mode = 'train')
    valid_dataset = FeedbackPrize2_Dataset(cfg, val, mode = 'valid')

    train_dataloader = DataLoader(train_dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = True, collate_fn = collate_fn)
    valid_dataloader = DataLoader(valid_dataset, batch_size = cfg.batch_size * 8, num_workers = cfg.num_workers, shuffle = False, collate_fn = collate_fn)

    logging.info('Preparing model, optimizer, and scheduler...')
    model = FeedbackPrize2_Model(cfg).to(cfg.device)
    optimizer = get_optimizer(cfg, model)
    num_training_steps = int(len(train_dataloader) * cfg.nepochs)
    scheduler = get_scheduler(cfg, optimizer, num_training_steps)

    best_score = np.inf
    if cfg.mode == 'train':
        for epoch in range(cfg.nepochs):
            start_time = time.time()
            
            if epoch == 4:
                break

            if epoch < cfg.start_swa_epoch:
                # Train
                best_score, valid = train_fn(cfg, model, train_dataloader, optimizer, epoch, num_training_steps, scheduler, 
                                             valid_dataloader, val, valid, best_score = best_score, fold = fold)
            else:
                if epoch == cfg.start_swa_epoch:
                    logging.info(' Enable SWA '.center(50, '-'))
                    swa_model = AveragedModel(model)
                    swa_scheduler = SWALR(optimizer, anneal_strategy = cfg.swa_anneal_strategy, anneal_epochs = cfg.swa_anneal_epochs, swa_lr = cfg.swa_lr)
                best_score, valid = train_fn(cfg, model, train_dataloader, optimizer, epoch, num_training_steps, scheduler, 
                                             valid_dataloader, val, valid, swa_model = swa_model, swa_scheduler = swa_scheduler, 
                                             best_score = best_score, fold = fold)
    else:
        ckp = torch.load(os.path.join(cfg.model_dir, cfg.model_name.split('_')[0][:-1], cfg.model_name.split('_')[0][-1], f'fold_{fold}.pt'), map_location = cfg.device)
        if cfg.use_swa:
            swa_model = AveragedModel(model)
            swa_model.load_state_dict(ckp['state_dict'])
            _, pred = valid_fn(cfg, swa_model, valid_dataloader)
        else:
            model.load_state_dict(ckp['state_dict'])
            _, pred = valid_fn(cfg, model, valid_dataloader)
        _, pred = valid_fn(cfg, model, valid_dataloader)
        true = np.array([cfg.label_map[item] for sublist in [i.split('|') for i in val['label_list'].tolist()] for item in sublist])
        assert pred.shape[0] == len(true)
        valid[list(cfg.label_map.keys())[:3]] = pred

        # Scoring
        best_score = metric(pred, true)

    del model, optimizer, scheduler
    torch.cuda.empty_cache()
    gc.collect()
    
    return best_score, valid

"""# Main"""

def main():
    oofs = []
    for fold in cfg.training_folds:
        fold_score, oof = training_loop(cfg, fold)
        logging.info(f' Score: {fold_score} '.center(50, '*'))
        oofs.append(oof)
    oofs = pd.concat(oofs)
    if set(cfg.training_folds) == {0, 1, 2, 3, 4}:
        oofs = oofs.loc[train.index]
    pred = oofs[list(cfg.label_map.keys())[:3]].values
    true = oofs['discourse_effectiveness'].map(cfg.label_map).values
    assert pred.shape[0] == len(true)
    # Scoring
    score = metric(pred, true)
    logging.info('=' * 50)
    logging.info(f' OOF Score: {score} '.center(50, '*'))
    # Storing OOF file
    oofs.to_pickle(os.path.join(cfg.model_dir, cfg.model_name.split('_')[0][:-1], cfg.model_name.split('_')[0][-1], f"oof_{''.join([str(i) for i in cfg.training_folds])}.pkl"))

if __name__ == '__main__':
    main()